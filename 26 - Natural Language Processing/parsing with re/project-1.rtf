{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red164\green191\blue255;\red23\green23\blue23;\red255\green255\blue255;
\red252\green115\blue96;\red129\green131\blue134;\red254\green219\blue112;\red117\green255\blue242;}
{\*\expandedcolortbl;;\cssrgb\c70196\c80000\c100000;\cssrgb\c11765\c11765\c11765;\cssrgb\c100000\c100000\c100000;
\cssrgb\c100000\c53725\c45098;\cssrgb\c57647\c58431\c59608;\cssrgb\c100000\c87843\c51373;\cssrgb\c51373\c100000\c96078;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28300\viewh17700\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \'97
\f1\fs28\fsmilli14080 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 nltk\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 pos_tag\cf4 \strokec4 , \cf5 \strokec5 RegexpParser\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 tokenize_words\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 word_sentence_tokenize\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 chunk_counters\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 np_chunk_counter\cf4 \strokec4 , \cf5 \strokec5 vp_chunk_counter\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # import text of choice here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 text\cf4 \strokec4  = open(\cf7 \strokec7 "the_iliad.txt"\cf4 \strokec4 ,\cf5 \strokec5 encoding\cf4 \strokec4 =\cf7 \strokec7 'utf-8'\cf4 \strokec4 ).\cf8 \strokec8 read\cf4 \strokec4 ().\cf8 \strokec8 lower\cf4 \strokec4 ()\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # sentence and word tokenize text here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 word_tokenized_text\cf4 \strokec4 =\cf5 \strokec5 word_sentence_tokenize\cf4 \strokec4 (\cf5 \strokec5 text\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # store and print any word tokenized sentence here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 single_word_tokenized_sentence\cf4 \strokec4  = \cf5 \strokec5 word_tokenized_text\cf4 \strokec4 [\cf5 \strokec5 100\cf4 \strokec4 ]\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 #print(single_word_tokenized_sentence)\cf4 \cb1 \strokec4 \
\
\
\cf6 \cb3 \strokec6 # create a list to hold part-of-speech tagged sentences here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 pos_tagged_text\cf4 \strokec4 =[]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # create a for loop through each word tokenized sentence here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 for\cf4 \strokec4  \cf5 \strokec5 token\cf4 \strokec4  \cf2 \strokec2 in\cf4 \strokec4  \cf5 \strokec5 word_tokenized_text\cf4 \strokec4 :\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf4 \cb3   \cf6 \strokec6 # part-of-speech tag each sentence and append to list of pos-tagged sentences here\cf4 \cb1 \strokec4 \
\cb3   \cf5 \strokec5 pos_tagged_text\cf4 \strokec4 .\cf8 \strokec8 append\cf4 \strokec4 (\cf5 \strokec5 pos_tag\cf4 \strokec4 (\cf5 \strokec5 token\cf4 \strokec4 ))\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # store and print any part-of-speech tagged sentence here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 single_pos_sentence\cf4 \strokec4 =\cf5 \strokec5 pos_tagged_text\cf4 \strokec4 [\cf5 \strokec5 100\cf4 \strokec4 ]\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 #print(single_pos_sentence)\cf4 \cb1 \strokec4 \
\
\cf6 \cb3 \strokec6 # define noun phrase chunk grammar here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 np_chunk_grammar\cf4 \strokec4 = \cf7 \strokec7 "NP: \{<DT>?<JJ>*<NN>\}"\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # create noun phrase RegexpParser object here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 np_chunk_parser\cf4 \strokec4 =\cf5 \strokec5 RegexpParser\cf4 \strokec4 (\cf5 \strokec5 np_chunk_grammar\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # define verb phrase chunk grammar here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 vp_chunk_grammar\cf4 \strokec4 = \cf7 \strokec7 "VP: \{<DT>?<JJ>*<NN><VB.*><RB.?>?\}"\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # create verb phrase RegexpParser object here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 vp_chunk_parser\cf4 \strokec4 =\cf5 \strokec5 RegexpParser\cf4 \strokec4 (\cf5 \strokec5 vp_chunk_grammar\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 np_chunked_text\cf4 \strokec4 =[]\cb1 \
\cf5 \cb3 \strokec5 vp_chunked_text\cf4 \strokec4 =[]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # create a for loop through each pos-tagged sentence here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 for\cf4 \strokec4  \cf5 \strokec5 sentence\cf4 \strokec4  \cf2 \strokec2 in\cf4 \strokec4  \cf5 \strokec5 pos_tagged_text\cf4 \strokec4 :\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf4 \cb3   \cf6 \strokec6 # chunk each sentence and append to lists here\cf4 \cb1 \strokec4 \
\cb3   \cf5 \strokec5 np_chunked_text\cf4 \strokec4 .\cf8 \strokec8 append\cf4 \strokec4 (\cf5 \strokec5 np_chunk_parser\cf4 \strokec4 .\cf8 \strokec8 parse\cf4 \strokec4 (\cf5 \strokec5 sentence\cf4 \strokec4 ))\cb1 \
\cb3   \cf5 \strokec5 vp_chunked_text\cf4 \strokec4 .\cf8 \strokec8 append\cf4 \strokec4 (\cf5 \strokec5 vp_chunk_parser\cf4 \strokec4 .\cf8 \strokec8 parse\cf4 \strokec4 (\cf5 \strokec5 sentence\cf4 \strokec4 ))\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # store and print the most common NP-chunks here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 most_common_np_chunks\cf4 \strokec4 = \cf5 \strokec5 np_chunk_counter\cf4 \strokec4 (\cf5 \strokec5 np_chunked_text\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf4 \cb3 print(\cf5 \strokec5 most_common_np_chunks\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf6 \cb3 \strokec6 # store and print the most common VP-chunks here\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 most_common_vp_chunks\cf4 \strokec4 = \cf5 \strokec5 vp_chunk_counter\cf4 \strokec4 (\cf5 \strokec5 vp_chunked_text\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf4 \cb3 print(\cf5 \strokec5 most_common_vp_chunks\cf4 \strokec4 )\cb1 \
}